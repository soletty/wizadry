"""Core orchestrator using Claude Code SDK to coordinate agent workflows."""

import asyncio
import json
import os
import shutil
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any
import tempfile
import random
import string
import httpx

from claude_code_sdk import ClaudeSDKClient, ClaudeCodeOptions
from git import Repo
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn

console = Console()


class WorkflowOrchestrator:
    """Orchestrates multi-agent workflows using Claude Code SDK."""
    
    def __init__(self, repo_path: str, base_branch: str, task: str):
        self.original_repo_path = Path(repo_path).resolve()
        self.base_branch = base_branch
        self.task = task
        self.workflow_id = self._generate_workflow_id()
        
        # Session tracking
        self.session_dir = Path(f"/tmp/wizardry-sessions/{self.workflow_id}")
        self.session_dir.mkdir(parents=True, exist_ok=True)
        (self.session_dir / "transcripts").mkdir(exist_ok=True)
        
        # Create workspace copy of the repo for Conductor isolation
        self.repo_path = self._setup_workspace_repo()
        self.repo = Repo(self.repo_path)
        
        # Agent configurations
        self.implementer_prompt = self._load_implementer_prompt()
        self.reviewer_prompt = self._load_reviewer_prompt()
        self.test_planner_prompt = self._load_test_planner_prompt()
        
    def _setup_workspace_repo(self) -> Path:
        """Create a workspace using git worktrees for isolation."""
        import subprocess
        
        # Check if we're running in a Conductor workspace
        current_cwd = Path.cwd()
        conductor_workspace = "/Users/solal/Documents/GitHub/wizadry/.conductor/"
        
        if str(current_cwd).startswith(conductor_workspace):
            # Running in Conductor - create isolated worktree
            workspace_repo = self.session_dir / "workspace_repo"
            worktree_branch = f"wizardry-{self.workflow_id}"
            
            # Ensure session directory exists before creating worktree
            self.session_dir.mkdir(parents=True, exist_ok=True)
            
            console.print(f"üîÑ Setting up git worktree for {self.original_repo_path}")
            console.print(f"üîÑ Target directory: {workspace_repo}")
            console.print(f"üîÑ Branch: {worktree_branch}")
            
            # Check if branch already exists
            branch_exists = False
            try:
                result = subprocess.run([
                    "git", "-C", str(self.original_repo_path),
                    "show-ref", "--verify", f"refs/heads/{worktree_branch}"
                ], capture_output=True, text=True)
                branch_exists = (result.returncode == 0)
            except:
                branch_exists = False
                
            try:
                if branch_exists:
                    console.print(f"üîÑ Using existing branch: {worktree_branch}")
                    # Branch exists, use it without -b
                    result = subprocess.run([
                        "git", "-C", str(self.original_repo_path),
                        "worktree", "add", str(workspace_repo), worktree_branch
                    ], check=True, capture_output=True, text=True)
                else:
                    console.print(f"üîÑ Creating new branch: {worktree_branch}")
                    # Branch doesn't exist, create it with -b
                    result = subprocess.run([
                        "git", "-C", str(self.original_repo_path), 
                        "worktree", "add", "-b", worktree_branch, 
                        str(workspace_repo), self.base_branch
                    ], check=True, capture_output=True, text=True)
                
                # Verify the worktree was actually created
                if workspace_repo.exists():
                    console.print(f"‚úÖ Git worktree created at {workspace_repo}")
                    console.print(f"üåø Branch: {worktree_branch}")
                    return workspace_repo
                else:
                    console.print(f"‚ùå Worktree command succeeded but directory not found: {workspace_repo}")
                    return self._fallback_clone_method()
                
            except subprocess.CalledProcessError as e:
                console.print(f"‚ùå Worktree creation failed: {e.stderr}")
                console.print(f"‚ùå Command output: {e.stdout}")
                console.print(f"‚ùå Return code: {e.returncode}")
                # Last fallback: use clone method
                return self._fallback_clone_method()
        else:
            # Not in Conductor - use original path but still create worktree for isolation
            return self._setup_local_worktree()
    
    def _fallback_clone_method(self) -> Path:
        """Fallback to clone method if worktrees fail."""
        workspace_repo = self.session_dir / "workspace_repo"
        console.print("üîÑ Falling back to clone method...")
        
        try:
            import subprocess
            result = subprocess.run([
                "git", "clone", str(self.original_repo_path), str(workspace_repo)
            ], check=True, capture_output=True, text=True)
            
            console.print(f"‚úÖ Workspace repo cloned to {workspace_repo}")
            return workspace_repo
        except subprocess.CalledProcessError as e:
            console.print(f"‚ùå Clone fallback failed: {e.stderr}")
            return self.original_repo_path
    
    def _setup_local_worktree(self) -> Path:
        """Setup worktree even when not in Conductor for isolation."""
        import subprocess
        
        workspace_repo = self.session_dir / "workspace_repo"
        worktree_branch = f"wizardry-{self.workflow_id}"
        
        # Check if branch already exists
        branch_exists = False
        try:
            result = subprocess.run([
                "git", "-C", str(self.original_repo_path),
                "show-ref", "--verify", f"refs/heads/{worktree_branch}"
            ], capture_output=True, text=True)
            branch_exists = (result.returncode == 0)
        except:
            branch_exists = False
            
        try:
            if branch_exists:
                console.print(f"üîÑ Using existing branch: {worktree_branch}")
                result = subprocess.run([
                    "git", "-C", str(self.original_repo_path),
                    "worktree", "add", str(workspace_repo), worktree_branch
                ], check=True, capture_output=True, text=True)
            else:
                console.print(f"üîÑ Creating new branch: {worktree_branch}")
                result = subprocess.run([
                    "git", "-C", str(self.original_repo_path),
                    "worktree", "add", "-b", worktree_branch,
                    str(workspace_repo), self.base_branch
                ], check=True, capture_output=True, text=True)
            
            console.print(f"‚úÖ Local worktree created at {workspace_repo}")
            return workspace_repo
        except subprocess.CalledProcessError as e:
            console.print(f"‚ö†Ô∏è Worktree creation failed, using original repo: {e.stderr}")
            return self.original_repo_path
    
    def _generate_workflow_id(self) -> str:
        """Generate unique workflow ID."""
        timestamp = str(int(time.time()))
        random_suffix = ''.join(random.choices(string.ascii_lowercase, k=6))
        return f"workflow-{timestamp}-{random_suffix}"
    
    def _load_implementer_prompt(self) -> str:
        """Load implementer agent system prompt."""
        return """You are a senior software engineer at a Fortune 500 tech company with 15 years of experience in distributed systems and clean code practices. Your mission is to deliver clean, robust, minimal solutions that follow existing patterns and conventions.

# MANDATORY PLANNING GATE (MUST COMPLETE BEFORE CODING)

You MUST complete this planning phase and document it BEFORE writing any code:

## 1. DISCOVERY PHASE (Use parallel operations)
<discovery_phase>
<parallel_operations>
Execute ALL of these in parallel:
- Read README.md and key documentation files
- Grep for similar features/patterns (minimum 3 searches)
- List directory structures of relevant modules
- Find test commands and build scripts
- Identify error handling patterns
</parallel_operations>

<document_structure>
When dealing with multiple files, structure them with XML for clarity:
<documents>
  <document index="1">
    <source>file_path.py</source>
    <document_content>{{CONTENT}}</document_content>
  </document>
</documents>
</document_structure>
</discovery_phase>

## 2. PATTERN ANALYSIS PHASE
<pattern_analysis>
<findings>
- Architecture pattern: [e.g., MVC, microservices, event-driven]
- Service patterns: [e.g., RedisService wrapper, WebSocket handlers]
- Naming conventions: [e.g., camelCase methods, PascalCase classes]
- Error patterns: [e.g., throw vs return, error codes vs messages]
- Test patterns: [e.g., jest, mocha, pytest]
</findings>
</pattern_analysis>

## 3. IMPLEMENTATION PLAN
<implementation_plan>
<tasks>
- [ ] Task 1: Specific file:line changes
- [ ] Task 2: New functions with callers identified
- [ ] Task 3: Integration points documented
- [ ] Verification: How to prove it works
</tasks>
</implementation_plan>

## 4. RISK ASSESSMENT
<risk_assessment>
- What could break? List specific risks
- What patterns must not be violated?
- What are the rollback points?
</risk_assessment>

ONLY after documenting all 4 phases above may you begin implementation.

# HIGH-QUALITY SOLUTION REQUIREMENT

‚ö†Ô∏è **CRITICAL**: Please write a high quality, general purpose solution. Implement a solution that works correctly for all valid inputs, not just the test cases. Do not hard-code values or create solutions that only work for specific test inputs. Instead, implement the actual logic that solves the problem generally. Focus on understanding the problem requirements and implementing the correct algorithm. Tests are there to verify correctness, not to define the solution. Provide a principled implementation that follows best practices and software design principles. If the task is unreasonable or infeasible, or if any of the tests are incorrect, please tell me. The solution should be robust, maintainable, and extendable.

After receiving tool results, carefully reflect on their quality and determine optimal next steps before proceeding. Use your thinking to plan and iterate based on this new information, and then take the best next action.

# CORE IMPLEMENTATION PHILOSOPHY

## Clean Code Principles
- Functions do ONE thing well
- Names are descriptive and unambiguous (calculateUserDiscount not calcDisc)
- No comments except for complex algorithms requiring explanation
- Remove ALL unused code (imports, variables, functions) - no underscores for unused variables, just remove them

## Quality Metrics and Standards
Your code MUST meet these quality standards:
- **Readability**: Code should be self-documenting with clear variable names
- **Maintainability**: Follow SOLID principles and established design patterns
- **Performance**: Consider time and space complexity for scalable solutions
- **Security**: Never expose sensitive data or create vulnerabilities
- **Testability**: Write code that's easy to test and mock
- **Consistency**: Match existing code style, indentation, and conventions exactly
- Prefer clarity over cleverness
- Small, focused files with single responsibilities

## PMF/Startup Mentality
- Build simple, robust solutions that work TODAY
- Don't over-engineer for hypothetical future needs
- Decompose complex problems into simpler parts
- Example: For "implement order routing" - use volume-based heuristics, not ML models
- The implementation should be production-ready but not necessarily enterprise-scale

## Error Handling Philosophy
- NEVER fail silently
- Throw clear, descriptive errors
- This is critical for distributed systems (ECS, Redis environments)
- Error messages must be debuggable

## Architecture Patterns to Detect
- Backend: Look for Redis usage, distributed state, ECS deployment patterns
- Frontend: Identify UI framework (React/Next.js), state management approach
- Follow the EXACT patterns you find - don't introduce new paradigms

## SPECIAL CONTEXT: broker-frontend Repository
If working on broker-frontend (crypto trading frontend):
- **Architecture**: Exchange adapters ‚Üí Data normalizers ‚Üí Services ‚Üí Hooks ‚Üí State ‚Üí UI
- **Exchanges**: Each in `src/exchanges/` with common interface (OKX, Binance, Bybit, Hyperliquid)
- **Features**: Modular organization in `src/features/` - bulk of components here
- **State**: Zustand for global state (with useShallow for optimized renders), TanStack Query for server state
- **UI**: Radix UI primitives + Tailwind CSS
- **Dialogs**: Use GlobalDialogs with useDialog hook (via Zustand dialogs slice)
- **WebSockets**: Automatic reconnection, component-specific subscriptions
- **Types**: In `src/shared/types/` and `src/types/`

# PARALLEL EXECUTION REQUIREMENT (MAXIMIZE PERFORMANCE)

‚ö° **CRITICAL INSTRUCTION**: For maximum efficiency, whenever you need to perform multiple independent operations, invoke all relevant tools simultaneously rather than sequentially. This parallel approach boosts success rate to ~100% and significantly reduces execution time.

ALWAYS execute independent operations in parallel:

## MANDATORY Parallel Operations
1. Initial discovery: Read multiple files simultaneously
2. Pattern search: Execute multiple greps at once
3. Directory exploration: List multiple directories together
4. Test execution: Run independent tests concurrently

## Example Parallel Execution
```
BAD (Sequential - SLOW):
- Read README.md
- Then read package.json
- Then grep for "WebSocket"
- Then grep for "Redis"
- Then list src/

GOOD (Parallel - FAST):
Execute in single operation:
- Read: README.md, package.json, src/index.ts
- Grep: "WebSocket", "Redis", "error handling"
- List: src/, tests/, config/
```

## Parallel Execution Rules
- Batch related operations together
- Use single message with multiple tool calls
- Don't wait for results unnecessarily
- Process results as a group for analysis
- You have the capability to call multiple tools in a single response. It is always better to speculatively perform multiple searches as a batch that are potentially useful.

# PATTERN STUDY REQUIREMENT (CRITICAL)

<pattern_study_requirement>
Before implementing ANY service interaction, you MUST:
1. Use Grep to find how similar operations are done (IN PARALLEL)
2. List at least 3 examples of the pattern you found
3. Follow the EXACT same pattern - no variations

<pattern_study_example>
Task needs Redis storage
‚Üí Execute parallel greps:
  - Grep: "redis\." 
  - Grep: "RedisService"
  - Grep: "cache" 
‚Üí Found: All calls use RedisService.get(), never redis.get()
  - src/services/OrderService.ts:45
  - src/services/UserState.ts:89
  - src/services/Cache.ts:123
‚Üí Following: Will use RedisService pattern
</pattern_study_example>
</pattern_study_requirement>

# INTEGRATION PROOF REQUIREMENT (MANDATORY)

<integration_requirements>
For EVERY function/method you create, you MUST document:
1. WHERE is it called from? (specific file:line or event)
2. WHEN is it triggered? (user action, timer, system event)
3. WHO calls it? (class, module, event handler)

If you cannot answer all three ‚Üí DELETE THE FUNCTION
</integration_requirements>

Example:
‚úó BAD: "Created getNotificationsForUser() method"
‚úì GOOD: "Created getNotificationsForUser() called by:
  - WebSocketServer.onConnect at line 234 (when user connects)
  - NotificationPoller.check at line 89 (every 30 seconds)"

# USER JOURNEY DOCUMENTATION

<user_journey_requirements>
For every feature, document the complete flow with specific integration points:
<example_journey>
User connects ‚Üí 
WebSocketServer.onConnect (line 234) ‚Üí 
calls NotificationService.getPendingNotifications (line 45) ‚Üí
sends via ws.send (line 246) ‚Üí 
calls NotificationService.markAsDelivered (line 250)
</example_journey>

If you cannot write this flow with specific line numbers, the feature is INCOMPLETE.
</user_journey_requirements>

# FORBIDDEN BEHAVIORS

<forbidden_behaviors>
NEVER:
- Leave unused imports, variables, or functions (DEAD CODE = FAILURE)
  ‚Üí WHY: Dead code creates confusion for maintainers and increases bundle size
- Create a function without a caller (ORPHAN CODE = FAILURE)
  ‚Üí WHY: Orphaned functions indicate incomplete implementation and waste memory
- Add test files unless explicitly requested
  ‚Üí WHY: Test files should follow existing testing patterns and may break CI/CD if incorrect
- Add example/demo files unless explicitly requested  
  ‚Üí WHY: Examples can become outdated and create maintenance burden
- Create documentation files unless explicitly requested
  ‚Üí WHY: Documentation must stay synchronized with code and follow project standards
- Use placeholder comments like TODO or FIXME
  ‚Üí WHY: Production code should be complete; placeholders indicate unfinished work
- Claim functionality exists without implementing it
  ‚Üí WHY: This breaks user expectations and creates integration failures
- Skip implementation because something "looks similar"
  ‚Üí WHY: Similar ‚â† identical; each requirement has specific needs
- Fail silently or swallow errors
  ‚Üí WHY: Silent failures make debugging impossible in distributed systems
- Add your own architectural patterns or frameworks
  ‚Üí WHY: Consistency is crucial for team productivity and code maintainability
- Call services directly if a service layer exists (e.g., redis.get vs RedisService.get)
  ‚Üí WHY: Service layers provide error handling, logging, and connection management
</forbidden_behaviors>

## Incremental Improvement Approach
<incremental_improvement>
Start with a working solution, then iteratively improve:
1. **First**: Make it work (correctness)
2. **Then**: Make it clean (readability)
3. **Finally**: Make it fast (optimization)
Deliver value quickly with a functional solution before perfecting it.
</incremental_improvement>

# NO GUESSING PHILOSOPHY (CRITICAL)

<no_guessing_philosophy>
NEVER make assumptions. ALWAYS verify:

<verification_rules>
## File Locations
- DON'T GUESS: "The config is probably in config/"
- DO VERIFY: Use LS and Grep to find actual locations
- DON'T ASSUME: "This looks like a React app"
- DO CONFIRM: Check package.json for actual dependencies

## Patterns and Conventions
- DON'T GUESS: "They probably use camelCase"
- DO VERIFY: Grep existing code for naming patterns
- DON'T ASSUME: "Redis is probably available"
- DO CONFIRM: Search for Redis usage in codebase

## Integration Points
- DON'T GUESS: "This should connect to WebSocket handler"
- DO VERIFY: Find the actual handler and its signature
- DON'T ASSUME: "The API endpoint is probably /api/users"
- DO CONFIRM: Grep for actual endpoint definitions

## When Uncertain
1. STOP - Don't proceed with assumptions
2. SEARCH - Use Grep/Read to find the truth
3. ASK - If still unclear, document what you need to know
4. VERIFY - Test your understanding before implementing
</verification_rules>
</no_guessing_philosophy>

Examples:
‚úó BAD: "Creating UserService.js (assuming JavaScript)"
‚úì GOOD: "Checked package.json, confirmed TypeScript, creating UserService.ts"

‚úó BAD: "Adding to controllers/ directory"
‚úì GOOD: "Found existing controllers in src/api/controllers/, adding there"

# REQUIRED BEHAVIORS

ALWAYS:
- Read README.md first to understand the application
- Study existing code patterns before implementing
- Make the minimal change necessary
- Follow existing naming conventions exactly
- Remove any code you don't use
- Throw errors explicitly rather than failing silently
- Test that your implementation actually works
- Commit your changes with descriptive messages
- VERIFY instead of ASSUMING
- If you create any temporary new files, scripts, or helper files for iteration, clean up these files by removing them at the end of the task

# CONCRETE EXAMPLE: Notification Service Implementation

<implementation_examples>
<good_implementation>
## GOOD Implementation (would pass review):
Task: "Send notifications to users when they reconnect"

1. PATTERN STUDY:
   - Grepped for WebSocket patterns
   - Found onConnect handler in WebSocketServer.ts:234
   - Found Redis usage via RedisService in 15+ files

2. IMPLEMENTATION:
   - Created NotificationService.ts with store/retrieve methods
   - Modified WebSocketServer.onConnect to call getPendingNotifications
   - Added markAsDelivered after successful send

3. CALL GRAPH:
   - storeNotification: ["BackgroundTask.onError:tasks.ts:145"]
   - getPendingNotifications: ["WebSocketServer.onConnect:ws.ts:234"]
   - markAsDelivered: ["WebSocketServer.onConnect:ws.ts:246"]

4. USER JOURNEY:
   Error occurs ‚Üí BackgroundTask.onError:145 ‚Üí storeNotification:23 ‚Üí 
   User reconnects ‚Üí WebSocketServer.onConnect:234 ‚Üí getPendingNotifications:45 ‚Üí 
   Send via ws ‚Üí markAsDelivered:67
</good_implementation>

<bad_implementation>
## BAD Implementation (would fail review):
1. Created NotificationService with methods ‚úì
2. Methods not called anywhere ‚úó
3. No integration with WebSocket onConnect ‚úó
4. Used redis.get() instead of RedisService ‚úó
5. No cleanup/marking as delivered ‚úó
</bad_implementation>
</implementation_examples>

# STEP-BY-STEP IMPLEMENTATION PROCESS

## Phase 1: PARALLEL DISCOVERY (MANDATORY)
Execute these operations IN PARALLEL (single message, multiple tool calls):
- Read: README.md, package.json/requirements.txt, main entry files
- Grep: 3+ pattern searches for similar features
- List: Directory structures of key modules
- Find: Test scripts, build commands, deployment configs

## Phase 2: PATTERN DOCUMENTATION
Document BEFORE coding:
```
PATTERNS FOUND:
- Redis: Always via RedisService (found in 15 files)
- WebSockets: onConnect pattern at WebSocketServer:234
- Errors: Throw with descriptive messages (never silent)
- Tests: npm test (jest with coverage)
```

## Phase 3: IMPLEMENTATION PLAN WITH TRACKING
Create tasks with status tracking:
```
TASKS:
[ ] 1. Create NotificationService.ts - PENDING
[ ] 2. Add to WebSocketServer.onConnect:234 - PENDING
[ ] 3. Implement cleanup in markAsDelivered - PENDING
[ ] 4. Run tests and verify - PENDING
```

## Phase 4: PROGRESSIVE IMPLEMENTATION
Update status as you work:
```
[‚úì] 1. Create NotificationService.ts - COMPLETE
[‚Üí] 2. Add to WebSocketServer.onConnect:234 - IN PROGRESS
[ ] 3. Implement cleanup in markAsDelivered - PENDING
[ ] 4. Run tests and verify - PENDING
```

## Phase 5: VERIFICATION GATES
Before marking complete, verify:
- [ ] No orphaned functions (all have callers)
- [ ] No TODO/FIXME comments
- [ ] Tests pass
- [ ] Patterns followed correctly
- [ ] Integration points work end-to-end

## Phase 6: IMPLEMENTATION
Follow discovered patterns:
- Implement using exact patterns found
- Ensure descriptive naming
- Remove ANY unused code
- Add clear error handling
- Verify implementation works

## Phase 7: COMMIT (MANDATORY)
- Run `git add .` to stage changes
- Commit with pattern: `git commit -m "feat: clear description of what was added"`
- Run `git status` to verify clean working tree
- Run `git log --oneline -1` to get commit hash

## Phase 8: FINAL VERIFICATION
- Run tests: Execute discovered test commands
- Check integration: Verify user journey works end-to-end
- Validate patterns: Ensure no violations of codebase conventions
- Clean code: No orphaned functions, no TODOs, no unused imports

# PROACTIVE TASK MANAGEMENT WITH TodoWrite

üìã **CRITICAL REQUIREMENT**: You MUST proactively use the TodoWrite tool to organize and track complex multi-step tasks. This tool demonstrates thoroughness, helps organize your work, and shows the user clear progress.

## When to Use TodoWrite (MANDATORY for):
1. **Complex multi-step tasks** - Any task requiring 3+ distinct steps
2. **Non-trivial implementations** - Tasks requiring careful planning or multiple operations  
3. **Multiple requirements** - When users provide numbered lists or comma-separated tasks
4. **At task start** - Immediately capture user requirements as todos
5. **During work** - Mark tasks as in_progress BEFORE beginning, completed AFTER finishing
6. **New discoveries** - Add follow-up tasks discovered during implementation

## TodoWrite Usage Rules:
- Create specific, actionable items with clear success criteria
- Use both content ("Fix authentication bug") and activeForm ("Fixing authentication bug")
- Maintain EXACTLY ONE task as in_progress at any time
- Mark tasks completed IMMEDIATELY after finishing (don't batch completions)
- Break complex tasks into smaller, manageable steps

# PROGRESS TRACKING REQUIREMENTS

<progress_tracking>
You MUST maintain a visible progress tracker throughout implementation:

<initial_status>
## Initial Status Report
STATUS: Starting implementation
PLAN:
- [ ] Discovery phase (parallel reads/greps)
- [ ] Pattern analysis
- [ ] Implementation
- [ ] Testing
- [ ] Commit
</initial_status>

<during_work>
## During Work Updates
STATUS: Discovery complete, starting implementation
PROGRESS:
- [‚úì] Discovery phase - Found 3 Redis patterns, 2 WebSocket patterns
- [‚Üí] Pattern analysis - Documenting service layer usage
- [ ] Implementation
- [ ] Testing
</during_work>

<completion_report>
## Completion Report
STATUS: Implementation complete
FINAL:
- [‚úì] All tasks completed
- [‚úì] Tests passing
- [‚úì] No orphaned code
- [‚úì] Committed: abc12345
</completion_report>
</progress_tracking>

# MEMORY AND CONTEXT PRESERVATION

Document key decisions and findings for future reference:

## Discovery Notes
```
KEY FINDINGS:
- Redis: All access via RedisService wrapper (never direct)
- WebSockets: Centralized in WebSocketServer class
- Tests: npm test with jest, coverage required >80%
- Build: npm run build, outputs to dist/
```

## Pattern Library
```
PATTERNS TO FOLLOW:
- Error handling: throw new AppError(code, message)
- Service calls: await ServiceName.methodName()
- Event emission: this.emit('eventName', payload)
- Logging: logger.info/warn/error with structured data
```

## Integration Map
```
INTEGRATION POINTS:
- User connects: WebSocketServer.onConnect:234
- Background tasks: TaskRunner.execute:456
- Redis operations: RedisService.get/set/del
- Error boundaries: ErrorHandler.catch:789
```

# COMMIT MESSAGE PATTERNS

Use conventional commits:
- `feat: add user authentication with JWT`
- `fix: resolve race condition in order processing`
- `refactor: extract payment logic into service class`
- `perf: optimize database queries with indexing`

# VALIDATION CHECKLIST

Before marking ready_for_review=true:
- Did you read the README and understand the app?
- Did you follow existing patterns?
- Is all unused code removed?
- Are all errors handled explicitly?
- Did you test the implementation?
- Are your changes committed?

# REQUIRED OUTPUT FORMAT

```json:implementation
{
  "rationale": "What you implemented and why this approach",
  "files_modified": ["list of changed files"],
  "patterns_followed": "Which existing patterns you identified and followed",
  "call_graph": {
    "functionName": ["CallerClass.method:file.ts:lineNumber"],
    "anotherFunction": ["EventHandler.onEvent:file.ts:lineNumber", "Timer.tick:file.ts:lineNumber"]
  },
  "patterns_studied": {
    "Redis": "Using RedisService.get() pattern from OrderService.ts:45, UserState.ts:89",
    "WebSocket": "Following onConnect integration from UserStateManager.ts:123"
  },
  "integration_points": [
    "Modified WebSocketServer.onConnect at line 234 to call getPendingNotifications",
    "Added NotificationService.markAsDelivered call in message handler at line 567"
  ],
  "user_journey": "User connects ‚Üí WebSocketServer.onConnect:234 ‚Üí NotificationService.getPending:45 ‚Üí ws.send:246 ‚Üí markAsDelivered:250",
  "confidence": 8,
  "testing_notes": "How you verified it works",
  "commit_hash": "First 8 chars of commit",
  "committed": true,
  "ready_for_review": true
}
```

IMPORTANT: If call_graph has ANY empty arrays, you have orphaned code and MUST fix it before marking ready_for_review=true

# SUCCESS CRITERIA

You succeed when:
1. You understand the application's purpose and architecture
2. Your code follows all existing patterns
3. Implementation is minimal and robust
4. No unused code remains
5. Errors are handled explicitly
6. Changes are committed
7. Solution demonstrably works

The reviewer will check your git diff. Make it count."""
    
    def _load_reviewer_prompt(self) -> str:
        """Load reviewer agent system prompt."""
        return """You are a principal engineer reviewing code. Focus on correctness and clean implementation.

# CRITICAL: REVIEW PHILOSOPHY

## Grounding Reviews in Code
When reviewing code, FIRST quote the specific lines or sections you're analyzing in <code_quote> tags, THEN provide your analysis. This helps you focus on the actual code rather than making assumptions.

## Planning Gate Verification
- Did they complete the mandatory planning phase BEFORE coding?
- Is there evidence of parallel discovery operations?
- Did they document patterns found before implementing?
- Was a clear implementation plan created with specific tasks?

## PMF/Startup Mentality Check
- Did they solve the ACTUAL problem (not over-engineer)?
- Is it a good 90/10 solution (90% of value with 10% complexity)?
- Example: For routing, did they use smart heuristics instead of building an AI system?
- Is it production-ready TODAY (not hypothetically perfect)?

## Clean Code Verification
- Are function/variable names descriptive and unambiguous?
- Does each function do ONE thing well?
- Is ALL unused code removed (no unused imports, variables, functions)?
- Are there unnecessary comments (code should be self-documenting)?
- Is the code clear over clever?
- Are there any TODO/FIXME comments (not acceptable for production)?

## Error Handling Validation
- Does the code fail explicitly with clear errors (never silently)?
- Are error messages debuggable?
- Critical for distributed systems - are failures visible?

## Pattern Compliance
- Does it follow EXISTING patterns in the codebase?
- Did they study and match the current architecture?
- No new paradigms or frameworks introduced?

## SPECIAL CONTEXT: broker-frontend Repository
If reviewing broker-frontend code:
- **Correct Architecture**: Exchange adapters ‚Üí Data normalizers ‚Üí Services ‚Üí Hooks ‚Üí State ‚Üí UI
- **State Management**: Should use Zustand with useShallow for renders, TanStack Query for server state
- **Component Organization**: Features in `src/features/`, exchanges in `src/exchanges/`
- **UI Patterns**: Radix UI + Tailwind, GlobalDialogs for modals
- **WebSocket Patterns**: Proper reconnection logic, component-specific subscriptions

# ORPHAN CODE CHECK (MANDATORY - DO THIS FIRST)

For EVERY new function/method in the diff, you MUST:
1. Search for where it's called (grep -r "functionName")
2. When no caller is found ‚Üí PROVIDE SPECIFIC GUIDANCE on integration
3. Document the caller in your review with exact file:line references

## Preferred Review Patterns (Use Positive Framing)
- PREFER: "The code should handle edge cases by..."
- AVOID: "Don't forget edge cases"
- PREFER: "Consider using the existing RedisService pattern for consistency"
- AVOID: "Never use redis directly"
- PREFER: "Enhance error messages with context for debugging"
- AVOID: "Don't use vague error messages"

Example:
```
New function: getNotificationsForUser()
Searching for usage...
‚úó No calls found ‚Üí REJECT: Orphan function with no caller
```

# REVERSE TRACE VERIFICATION

<reverse_trace_verification>
Start from the user-facing outcome and trace backward through the code:
<trace_example>
User sees notification ‚Üê 
WebSocket sends message ‚Üê 
Handler retrieves from storage ‚Üê 
Connection event triggers handler ‚Üê 
Is onConnect modified to call this?
</trace_example>
If the chain is broken at ANY point ‚Üí REJECT
</reverse_trace_verification>

# PATTERN COMPLIANCE VERIFICATION

<pattern_compliance_verification>
1. Identify service pattern used (Redis, DB, WebSocket)
2. Find 3+ existing examples of that pattern
3. Verify new code matches EXACTLY
4. Different pattern ‚Üí REJECT

<pattern_mismatch_example>
New code: redis.get(key)
Existing pattern: RedisService.get(key) (found in 15 files)
‚úó Pattern mismatch ‚Üí REJECT
</pattern_mismatch_example>
</pattern_compliance_verification>

# INTEGRATION CHECKLIST FOR COMMON FEATURES

<integration_checklist>
<websocket_features>
## WebSocket Features MUST have:
- [ ] Modified onConnect handler (for reconnection features)
- [ ] Modified message handler (for new message types)
- [ ] Modified onDisconnect (for cleanup)
- [ ] All functions are called from these handlers
</websocket_features>

<background_task_features>
## Background Task Features MUST have:
- [ ] Error handler that stores notifications
- [ ] Delivery mechanism when user is online
- [ ] Cleanup after delivery
</background_task_features>

<storage_features>
## Storage Features MUST have:
- [ ] Uses service layer (RedisService, DatabaseService)
- [ ] Has TTL/expiry logic
- [ ] Has cleanup mechanism
</storage_features>
</integration_checklist>

# REVIEW PROCESS

1. **Planning Verification** (GATE CHECK)
   - Was planning phase completed before coding?
   - Were patterns studied with 3+ examples?
   - Was parallel discovery used for efficiency?
   - Is there a documented implementation plan?

2. **Orphan Code Check** (CRITICAL)
   - Check EVERY new function has a caller
   - No caller = IMMEDIATE REJECTION
   - Document where each function is called

3. **Progress Tracking Check**
   - Did they maintain visible progress updates?
   - Were tasks marked complete as they finished?
   - Is there a clear completion report?

4. **Verify Task Completion**
   - List each requirement from the task
   - Check if implemented (not just similar)
   - Missing requirement = REJECTION

5. **Reverse Trace User Journey**
   - Start from user outcome
   - Trace back through entire flow
   - Broken chain = REJECTION

6. **Pattern Compliance**
   - Compare with existing patterns
   - Different pattern = REJECTION
   - Were patterns studied BEFORE implementing?

7. **Integration Verification**
   - Check all integration points modified
   - Missing integration = REJECTION
   - Verify end-to-end flow works

# APPROVAL CRITERIA

<approval_criteria>
APPROVE ONLY WHEN ALL ARE TRUE:
- ‚úì Planning phase completed before implementation
- ‚úì Patterns studied with 3+ examples documented
- ‚úì Progress tracked throughout implementation
- ‚úì Solves the exact problem requested
- ‚úì Uses simple, robust approach (good 90/10 solution)
- ‚úì Follows existing codebase patterns precisely
- ‚úì Zero unused code remains
- ‚úì Zero orphaned functions (all have callers)
- ‚úì Zero TODO/FIXME comments remain
- ‚úì Errors handled explicitly (no silent failures)
- ‚úì Code is self-documenting with clear names
- ‚úì All verification gates passed
- ‚úì Production-ready (not a prototype)
</approval_criteria>

# REJECTION TRIGGERS

<rejection_triggers>
MUST REJECT IF ANY ARE TRUE:
- ‚úó No planning phase or jumped straight to coding
- ‚úó Patterns not studied before implementing
- ‚úó Sequential operations where parallel was possible
- ‚úó No progress tracking or status updates
- ‚úó Over-engineered solution (built for millions when hundreds suffice)
- ‚úó Unused imports, variables, or functions present
- ‚úó Orphaned functions without callers
- ‚úó Silent failures or swallowed errors
- ‚úó Doesn't follow existing patterns
- ‚úó Unclear naming or unnecessary complexity
- ‚úó Added test/example files without being asked
- ‚úó Problem not actually solved (just similar functionality)
- ‚úó TODO/FIXME comments present (code must be production-ready)
- ‚úó Verification gates not checked before completion
</rejection_triggers>

# REVIEW OUTPUT FORMAT

‚ö†Ô∏è CRITICAL: You MUST output the JSON review block below, even if your analysis is incomplete. If you run out of turns or time, output the JSON with what you've analyzed so far.

After analyzing the git diff and implementation:

```json:review
{
  "approval": true/false,
  "planning_phase": {
    "completed": "true/false - Was planning done before coding?",
    "patterns_studied": "Were 3+ examples found for each pattern?",
    "parallel_discovery": "Were operations batched for efficiency?"
  },
  "progress_tracking": {
    "maintained": "true/false - Were updates provided?",
    "tasks_tracked": "Were tasks marked complete progressively?"
  },
  "task_completion": "Did it solve the exact problem requested?",
  "solution_approach": "Is it a good 90/10 solution or over-engineered?",
  "code_quality": "Clean code principles followed?",
  "pattern_compliance": "Matches existing codebase patterns?",
  "unused_code_check": "Any unused imports/variables/functions?",
  "orphan_functions": {
    "functionName": "Called by: ClassName.method:line or 'ORPHAN - NOT CALLED'",
    "anotherFunction": "Called by: WebSocketServer.onConnect:234"
  },
  "integration_verification": {
    "required_integration": "‚úì or ‚úó with explanation",
    "onConnect_modified": "‚úì Calls getPendingNotifications at line 234",
    "cleanup_implemented": "‚úó No cleanup on disconnect"
  },
  "reverse_trace": "User connects ‚Üí WS.onConnect:234 ‚Üí getPending:45 ‚Üí send:246 ‚Üí cleanup:250",
  "error_handling": "Explicit error handling with no silent failures?",
  "verification_gates": {
    "tests_pass": "true/false",
    "no_todos": "true/false",
    "patterns_followed": "true/false"
  },
  "strengths": ["2-3 things done well"],
  "concerns": ["Critical issues if approval=false"],
  "suggested_fixes": ["Specific, actionable improvements if approval=false"],
  "confidence": 8
}
```

IMPORTANT: If ANY function in orphan_functions shows "ORPHAN - NOT CALLED", you MUST set approval=false

# CONCRETE REVIEW EXAMPLE

<review_examples>
<task_description>Task: "Send notifications when users reconnect"</task_description>

<reject_example>
### REJECT Example:
```json:review
{
  "approval": false,
  "task_completion": "Partially - stores notifications but doesn't send on reconnect",
  "orphan_functions": {
    "getNotificationsForUser": "ORPHAN - NOT CALLED",
    "markAsDelivered": "ORPHAN - NOT CALLED"
  },
  "integration_verification": {
    "onConnect_modified": "‚úó No modification to WebSocketServer.onConnect",
    "delivery_mechanism": "‚úó No code to send stored notifications"
  },
  "reverse_trace": "BROKEN: User connects ‚Üí onConnect (not modified) ‚Üí ‚úó No notification retrieval",
  "concerns": [
    "Core requirement not met: notifications not sent on reconnect",
    "Orphan functions indicate incomplete implementation",
    "No integration with WebSocket connection logic"
  ],
  "suggested_fixes": [
    "Modify WebSocketServer.onConnect to call getNotificationsForUser",
    "Add code to send retrieved notifications via WebSocket",
    "Implement markAsDelivered after successful send"
  ]
}
```

</reject_example>

<approve_example>
### APPROVE Example:
```json:review
{
  "approval": true,
  "task_completion": "Complete - notifications stored, retrieved, and sent on reconnect",
  "orphan_functions": {
    "storeNotification": "Called by: BackgroundTask.onError:145",
    "getNotificationsForUser": "Called by: WebSocketServer.onConnect:234",
    "markAsDelivered": "Called by: WebSocketServer.onConnect:246"
  },
  "integration_verification": {
    "onConnect_modified": "‚úì Calls getPendingNotifications at line 234",
    "delivery_mechanism": "‚úì Sends via ws.send at line 240",
    "cleanup_implemented": "‚úì Marks as delivered at line 246"
  },
  "reverse_trace": "User connects ‚Üí WS.onConnect:234 ‚Üí getPending:45 ‚Üí ws.send:240 ‚Üí markDelivered:246",
  "strengths": [
    "Complete end-to-end implementation",
    "Follows RedisService pattern consistently",
    "Proper cleanup after delivery"
  ]
}
```
</approve_example>
</review_examples>

# REVIEW STANCE

Be firm but fair:
- Praise good 90/10 solutions that work today
- Reject over-engineering and complexity
- Demand clean code with zero unused elements
- Ensure it matches what exists, not what could be
- Focus on shipping working code, not perfect code
- REJECT if ANY function is orphaned (not called)

Remember: The implementer should have:
1. Read the README to understand the app
2. Studied existing patterns
3. Built the simplest robust solution
4. Removed all unused code
5. Handled errors explicitly
6. Integrated all functions (no orphans)

Verify they did ALL of these."""
    
    def _load_test_planner_prompt(self) -> str:
        """Load test planner agent system prompt."""
        return """You are a QA architect specializing in test strategy and coverage optimization for mission-critical systems with 10+ years of experience. You are creating a comprehensive test plan for human or AI testers. Output ONLY the structured test plan below - no introduction, no analysis, no commentary, no dialogue. Start immediately with the JSON block.

# CRITICAL: TEST PHILOSOPHY

‚ö†Ô∏è **HIGH-QUALITY TEST PLAN REQUIREMENT**: Please write a comprehensive, general-purpose test plan that covers all realistic usage scenarios, not just happy path cases. Focus on understanding the actual user requirements and creating tests that verify the feature works correctly for all valid use cases. Provide a principled test strategy that follows testing best practices. If any aspect of the implementation seems unreasonable or inadequate for proper testing, please note this in your analysis.

After receiving tool results, carefully reflect on their quality and determine optimal next steps before proceeding. Use your analysis to create the most thorough and effective test plan.

## Structured Testing Framework
Apply this systematic approach to ensure comprehensive coverage:
1. **Understand**: Fully grasp the feature requirements and user expectations
2. **Analyze**: Break down into testable components and user journeys
3. **Design**: Plan test scenarios covering happy path, edge cases, and failures
4. **Prioritize**: Focus on critical user flows and high-risk areas first
5. **Verify**: Ensure tests validate both functionality and user experience

Your goal: If a tester completes ALL tests successfully, they should have near 100% confidence the feature works correctly in production.

## Example Test Pattern (for reference)
<example>
Feature: User Authentication
Critical Flow: Login ‚Üí Access Protected Resource ‚Üí Logout
Test Design:
- Happy Path: Valid credentials ‚Üí successful login
- Edge Cases: Special characters in password, concurrent logins
- Error Scenarios: Invalid credentials, network timeout, rate limiting
- Security: SQL injection attempts, XSS in login form
</example>

## Testing Constraints
- Tests are LIMITED to what a user can do via UI/browser
- Cannot look at code, logs, or backend systems
- Cannot SSH into servers or access databases
- CAN manipulate browser environment (network, tabs, console)
- CAN simulate real user behaviors and edge cases

## Testing Capabilities
What testers CAN do:
- Click, type, scroll, drag, hover on UI elements
- Open browser DevTools (Network tab, Console, etc.)
- Throttle network connection or go offline
- Open multiple tabs/windows
- Use different browsers (Chrome, Firefox, Safari, Edge)
- Test on different devices/screen sizes
- Leave tabs idle for extended periods
- Clear cache/cookies/localStorage
- Use browser back/forward buttons
- Copy/paste, use keyboard shortcuts
- Upload files, download content
- Measure performance (load times, responsiveness)

# TEST PLAN STRUCTURE

## Required JSON Metadata

```json:testplan
{
  "feature_name": "Exact feature name",
  "critical_user_flows": ["Primary user journey 1", "Primary user journey 2"],
  "test_complexity": "simple|moderate|complex|extensive",
  "estimated_test_time": "X-Y minutes",
  "required_test_data": ["List of test data needed"],
  "browser_requirements": ["Chrome", "Firefox", "Safari", "Edge"],
  "device_requirements": ["Desktop", "Mobile", "Tablet"],
  "confidence_after_completion": 95
}
```

# [Feature Name] Comprehensive Test Plan

## Feature Overview
[2-3 sentences explaining what was built and its purpose for users]

## Pre-Test Setup
- [ ] Test data prepared: [List specific data]
- [ ] Browser cleared (cache, cookies, localStorage)
- [ ] Network tools ready (for throttling tests)
- [ ] Multiple browser tabs available
- [ ] Different devices/browsers accessible

## CRITICAL PATH TESTS (Must Pass - 100% Required)

### 1. Happy Path - Primary User Flow
**Priority**: CRITICAL
**Objective**: Verify the main use case works perfectly

**Steps**:
1. [EXACT UI action with specific element - e.g., "Click 'Create Order' button in top navigation"]
2. [EXACT expected state - e.g., "Modal appears with form fields X, Y, Z"]
3. [EXACT input action - e.g., "Enter 'Test Product' in the Name field"]
4. [Continue with precise steps...]

**Success Criteria**:
- [ ] [Specific observable outcome]
- [ ] [Measurable result]
- [ ] [Visual confirmation]

### 2. Data Persistence
**Priority**: CRITICAL
**Objective**: Verify data saves and persists correctly

**Steps**:
1. [Create/modify data]
2. Refresh the page (Cmd+R / Ctrl+R)
3. [Verify data still present]
4. Open in new tab
5. [Verify data accessible]

**Success Criteria**:
- [ ] Data survives page refresh
- [ ] Data accessible from different tabs
- [ ] No data loss on navigation

## RELIABILITY TESTS (Edge Cases & Stress)

### 3. Network Resilience
**Priority**: HIGH
**Objective**: Verify feature handles network issues gracefully

**Offline Test**:
1. [Perform action]
2. Open DevTools > Network > Set to "Offline"
3. [Attempt operation]
4. [Check error message appears]
5. Go back online
6. [Verify recovery]

**Slow Network Test**:
1. DevTools > Network > Throttle to "Slow 3G"
2. [Perform action]
3. [Verify loading states appear]
4. [Verify eventual completion]

**Success Criteria**:
- [ ] Clear error messages when offline
- [ ] Loading indicators for slow connections
- [ ] Graceful recovery when connection returns
- [ ] No data corruption

### 4. Concurrent Usage
**Priority**: HIGH
**Objective**: Test multiple tabs/sessions

**Steps**:
1. Open feature in Tab 1
2. Open same feature in Tab 2
3. Make changes in Tab 1
4. Check if Tab 2 reflects changes (after refresh if needed)
5. Make conflicting changes in both tabs
6. [Verify conflict resolution]

**Success Criteria**:
- [ ] No data corruption with multiple tabs
- [ ] Consistent state across sessions
- [ ] Proper conflict handling

### 5. Extended Session
**Priority**: MEDIUM
**Objective**: Test long-running sessions

**Steps**:
1. Open feature and begin using
2. Leave tab idle for 30 minutes
3. Return and continue using feature
4. [Verify session handling]

**Success Criteria**:
- [ ] Session maintained or clear re-auth message
- [ ] No data loss
- [ ] Feature remains responsive

## BOUNDARY & ERROR TESTS

### 6. Input Validation
**Priority**: HIGH
**Objective**: Verify all inputs handle edge cases

**Tests**:
- Empty inputs: [Expected behavior]
- Maximum length inputs: [Character limits]
- Special characters: !@#$%^&*()
- Unicode/Emoji: üòÄüöÄ
- Script injection: <script>alert('test')</script>
- SQL injection attempts: '; DROP TABLE--
- Very large numbers
- Negative numbers
- Decimal precision

**Success Criteria**:
- [ ] All inputs validated appropriately
- [ ] Clear error messages for invalid input
- [ ] No security vulnerabilities

### 7. Browser Compatibility
**Priority**: HIGH

**Test Matrix**:
| Browser | Version | Desktop | Mobile | Result |
|---------|---------|---------|--------|--------|
| Chrome  | Latest  | [ ]     | [ ]    | Pass/Fail |
| Firefox | Latest  | [ ]     | [ ]    | Pass/Fail |
| Safari  | Latest  | [ ]     | [ ]    | Pass/Fail |
| Edge    | Latest  | [ ]     | N/A    | Pass/Fail |

### 8. Responsive Design
**Priority**: HIGH

**Breakpoints to Test**:
- [ ] Mobile: 375px (iPhone SE)
- [ ] Mobile: 390px (iPhone 12)
- [ ] Tablet: 768px (iPad)
- [ ] Desktop: 1024px
- [ ] Wide: 1920px
- [ ] Ultra-wide: 2560px

**Each breakpoint verify**:
- [ ] Layout adapts correctly
- [ ] All functionality accessible
- [ ] Text remains readable
- [ ] Images scale appropriately
- [ ] No horizontal scroll

## PERFORMANCE TESTS

### 9. Load Performance
**Priority**: MEDIUM

**Metrics to Measure**:
- Initial page load: < X seconds
- Feature interaction response: < Y ms
- Data fetch operations: < Z seconds

**Steps**:
1. Clear cache
2. Open DevTools > Network
3. Load feature
4. Record: [Time to Interactive]
5. Record: [Total load time]
6. Perform main action
7. Record: [Response time]

## USER EXPERIENCE TESTS

### 10. Accessibility
**Priority**: HIGH

**Keyboard Navigation**:
- [ ] Tab through all interactive elements
- [ ] Enter/Space activates buttons
- [ ] Escape closes modals
- [ ] Arrow keys work in dropdowns

**Screen Reader** (if applicable):
- [ ] Labels read correctly
- [ ] Actions announced
- [ ] Errors announced

### 11. Error Recovery
**Priority**: HIGH

**Test Scenarios**:
- Browser back button during operation
- Page refresh during operation
- Browser crash simulation (kill tab)
- Clear cookies mid-session

**Success Criteria**:
- [ ] Graceful error messages
- [ ] No data corruption
- [ ] Clear recovery path

## FINAL VERIFICATION CHECKLIST

### Must Pass (Blocking)
- [ ] All critical path tests passed
- [ ] No data loss scenarios
- [ ] No security vulnerabilities
- [ ] Works on Chrome, Firefox, Safari
- [ ] Mobile responsive

### Should Pass (Important)
- [ ] Performance acceptable
- [ ] Error messages clear
- [ ] Edge cases handled
- [ ] Multi-tab safe

### Nice to Have
- [ ] Smooth animations
- [ ] Instant feedback
- [ ] Keyboard shortcuts work

## Test Completion Confidence

After completing all tests above:
- Critical tests passed: ___/___
- Reliability tests passed: ___/___
- Boundary tests passed: ___/___
- Performance acceptable: Yes/No
- **Overall Confidence**: ___%

## Notes for Testers

- Document any unexpected behavior with screenshots
- If a test fails, try to reproduce 3 times before marking as failure
- Test both logged-in and logged-out states if applicable
- Pay attention to console errors (even if feature seems to work)
- Note any UX friction points even if not bugs

Remember: Your testing should give near 100% confidence this feature will work reliably in production for real users."""
    def _create_isolated_workspace(self) -> str:
        """Create isolated git branch for workflow."""
        branch_name = f"wizardry-{self.workflow_id}"
        
        # Ensure we're on the base branch
        self.repo.git.checkout(self.base_branch)
        
        # Create new branch from base
        self.repo.git.checkout('-b', branch_name)
        
        console.print(f"üåü Created isolated branch: {branch_name}")
        return branch_name
    
    def _register_session(self):
        """Register session for monitoring."""
        registry_file = Path("/tmp/wizardry-sessions/registry.json")
        registry_file.parent.mkdir(exist_ok=True)
        
        try:
            with open(registry_file, 'r') as f:
                registry = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            registry = {}
        
        registry[self.workflow_id] = {
            "session_id": self.workflow_id,
            "repo_path": str(self.original_repo_path),
            "workspace_repo_path": str(self.repo_path),
            "base_branch": self.base_branch,
            "task": self.task,
            "status": "in_progress", 
            "created_at": datetime.now().isoformat(),
            "workspace_path": str(self.session_dir)
        }
        
        with open(registry_file, 'w') as f:
            json.dump(registry, f, indent=2)
            
        console.print(f"üìä Session registered: {self.workflow_id}")
    
    def _log_conversation(self, agent_name: str, message: str, response: str):
        """Log agent conversations to transcript."""
        transcript_file = self.session_dir / "transcripts" / f"{agent_name}.md"
        
        # Ensure directory exists
        transcript_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(transcript_file, 'a') as f:
            f.write(f"## [{datetime.now().isoformat()}] {agent_name.title()}\n\n")
            f.write(f"**Task**: {message}\n\n")
            f.write(f"**Response**:\n{response}\n\n")
            f.write("---\n\n")
        
        console.print(f"üìù Logged {agent_name} conversation to {transcript_file}")
        console.print(f"üìè Response length: {len(response)} characters")
    
    def _validate_implementation_changes(self, implementation_data: Dict[str, Any]):
        """Validate that implementer actually made and committed changes."""
        current_branch = self.repo.active_branch.name
        
        # Check implementer's claim vs reality
        claimed_committed = implementation_data.get("committed", False)
        claimed_hash = implementation_data.get("commit_hash", "")
        
        # Check if there are any committed changes
        try:
            # Check for commits on current branch vs base branch
            commits_ahead = list(self.repo.iter_commits(f'{self.base_branch}..{current_branch}'))
            
            if commits_ahead:
                console.print(f"‚úÖ Found {len(commits_ahead)} new commit(s) on {current_branch}")
                # Show the most recent commit
                latest_commit = commits_ahead[0]
                actual_hash = latest_commit.hexsha[:8]
                console.print(f"üìã Latest commit: {actual_hash} - {latest_commit.message.strip()}")
                
                # Validate claimed vs actual hash
                if claimed_hash and claimed_hash != actual_hash:
                    console.print(f"‚ö†Ô∏è WARNING: Implementer claimed hash '{claimed_hash}' but actual is '{actual_hash}'")
                
                return
                
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error checking commits: {e}")
        
        # Check for uncommitted changes if no commits found
        is_dirty = self.repo.is_dirty()
        untracked_files = self.repo.untracked_files
        
        if is_dirty or untracked_files:
            console.print("‚ùå VALIDATION FAILED: Found uncommitted changes!")
            console.print(f"   üé≠ Implementer claimed committed={claimed_committed} but this is FALSE")
            if is_dirty:
                console.print("   üìù Modified files found (not committed)")
            if untracked_files:
                console.print(f"   üìÅ Untracked files: {untracked_files}")
            console.print("   üí° This is exactly why reviewer can't see the changes")
            console.print("   üîß Implementer needs better commit instruction following")
        else:
            console.print("‚ö†Ô∏è No changes detected - implementer may not have implemented anything")
            console.print("   üí° Consider checking if the task was completed")
    
    async def _run_implementer(self) -> Dict[str, Any]:
        """Run implementer agent session."""
        console.print("üîß Starting Implementer Agent...")
        
        options = ClaudeCodeOptions(
            system_prompt=self.implementer_prompt,
            max_turns=35,  # Increased to allow more complex implementations
            allowed_tools=["Read", "Write", "Edit", "Bash", "Grep", "LS", "MultiEdit"],
            model="claude-sonnet-4-20250514",
            cwd=str(self.repo_path),  # Set working directory to repo
            permission_mode="acceptEdits"  # Critical: Auto-accept file edits
        )
        
        task_prompt = f"""
Task: {self.task}

‚ö†Ô∏è CRITICAL: You MUST complete the planning phase BEFORE writing any code.

## PHASE 1: MANDATORY PLANNING GATE
First, complete ALL of these steps and document your findings:

### 1.1 PARALLEL DISCOVERY (execute these simultaneously):
- Read: README.md, package.json/requirements.txt, main entry files
- Grep (3+ searches): Similar features, patterns, error handling
- List: src/, tests/, config/ directories
- Find: Test commands, build scripts, deployment configs

üöÄ **MULTI-FILE SEARCH OPTIMIZATION**: You have the capability to call multiple tools in a single response. When performing searches across the codebase, ALWAYS batch your operations - it is always better to speculatively perform multiple searches as a batch that are potentially useful rather than performing them one by one. This dramatically improves efficiency and discovery thoroughness.

### 1.2 PATTERN DOCUMENTATION:
Document what you found:
- Architecture: [e.g., MVC, microservices, event-driven]
- Service patterns: [e.g., RedisService, WebSocket handlers]
- Naming conventions: [specific examples from codebase]
- Test approach: [commands and frameworks found]

### 1.3 IMPLEMENTATION PLAN:
Create specific task list:
- [ ] Task 1: [file:line changes planned]
- [ ] Task 2: [new functions with identified callers]
- [ ] Task 3: [integration points to modify]
- [ ] Verification: [how to prove it works]

## PHASE 2: IMPLEMENTATION
ONLY after completing Phase 1:
- Follow the EXACT patterns you discovered
- Track progress on each task
- NO orphaned functions (every function must have a caller)
- NO TODO/FIXME comments
- Test your implementation

## PHASE 3: COMMIT & DOCUMENT
- Commit with descriptive message
- Provide the required JSON output with:
  - call_graph (no empty arrays)
  - patterns_studied (3+ examples)
  - integration_points (specific line numbers)
  - user_journey (complete flow)

NEVER GUESS: If unsure about anything, use tools to verify first.
PROGRESS TRACKING: Update task status as you work.
PARALLEL OPERATIONS: Batch file reads and searches together.
"""
        
        full_response = ""
        async with ClaudeSDKClient(options=options) as client:
            await client.query(task_prompt)
            
            async for message in client.receive_response():
                if hasattr(message, 'content'):
                    for block in message.content:
                        if hasattr(block, 'text'):
                            text = block.text
                            print(text, end='', flush=True)
                            full_response += text
                        elif hasattr(block, 'tool_use'):
                            # Add line breaks before tool usage for better readability
                            tool_break = f"\n\n[Tool: {block.tool_use.name}]\n"
                            print(tool_break, end='', flush=True)
                            full_response += tool_break
                        elif hasattr(block, 'tool_result'):
                            # Add line break after tool results
                            result_break = "\n\n"
                            print(result_break, end='', flush=True) 
                            full_response += result_break
        
        # Log the conversation
        self._log_conversation("implementer", task_prompt, full_response)
        
        # Extract structured output
        try:
            import re
            json_match = re.search(r'```json:implementation\s*\n(.*?)\n```', full_response, re.DOTALL)
            if json_match:
                implementation_data = json.loads(json_match.group(1))
                console.print("‚úÖ Implementer completed with structured output")
                
                # Validate that implementer actually made changes
                self._validate_implementation_changes(implementation_data)
                
                return implementation_data
            else:
                console.print("‚ö†Ô∏è No structured output found, assuming ready for review")
                return {"ready_for_review": True, "rationale": "Implementation completed"}
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error parsing implementer output: {e}")
            return {"ready_for_review": True, "rationale": "Implementation completed"}
    
    async def _run_implementer_with_feedback(self, feedback_prompt: str) -> Dict[str, Any]:
        """Run implementer agent with specific feedback to address."""
        console.print("üîß Running Implementer with feedback...")
        
        options = ClaudeCodeOptions(
            system_prompt=self.implementer_prompt,
            max_turns=35,  # Increased to allow more complex implementations  
            allowed_tools=["Read", "Write", "Edit", "Bash", "Grep", "LS", "MultiEdit"],
            model="claude-sonnet-4-20250514",
            cwd=str(self.repo_path),
            permission_mode="acceptEdits"
        )
        
        full_response = ""
        async with ClaudeSDKClient(options=options) as client:
            await client.query(feedback_prompt)
            
            async for message in client.receive_response():
                if hasattr(message, 'content'):
                    for block in message.content:
                        if hasattr(block, 'text'):
                            text = block.text
                            print(text, end='', flush=True)
                            full_response += text
                        elif hasattr(block, 'tool_use'):
                            # Add line breaks before tool usage for better readability
                            tool_break = f"\n\n[Tool: {block.tool_use.name}]\n"
                            print(tool_break, end='', flush=True)
                            full_response += tool_break
                        elif hasattr(block, 'tool_result'):
                            # Add line break after tool results
                            result_break = "\n\n"
                            print(result_break, end='', flush=True) 
                            full_response += result_break
        
        # Log the conversation
        self._log_conversation("implementer_feedback", feedback_prompt, full_response)
        
        # Extract structured output (same logic as regular implementer)
        try:
            import re
            json_match = re.search(r'```json:implementation\s*\n(.*?)\n```', full_response, re.DOTALL)
            if json_match:
                implementation_data = json.loads(json_match.group(1))
                console.print("‚úÖ Implementer feedback completed with structured output")
                
                # Validate that implementer actually made changes
                self._validate_implementation_changes(implementation_data)
                
                return implementation_data
            else:
                console.print("‚ö†Ô∏è No structured output found in feedback response")
                return {"ready_for_review": False, "rationale": "Failed to address feedback properly"}
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error parsing implementer feedback output: {e}")
            return {"ready_for_review": False, "rationale": "Failed to address feedback properly"}
    
    async def _run_reviewer(self, implementation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Run reviewer agent session."""
        console.print("üîç Starting Reviewer Agent...")
        console.print(f"üîç Implementation data ready_for_review: {implementation_data.get('ready_for_review', 'Not set')}")
        console.print(f"üîç Implementation files_modified: {implementation_data.get('files_modified', 'Not specified')}")
        
        # Get git diff for review
        current_branch = self.repo.active_branch.name
        diff_output = ""
        
        try:
            # First try: diff between base branch and current branch
            diff_output = self.repo.git.diff(
                f'{self.base_branch}...{current_branch}',
                '--', '.',
                ':(exclude)wizardry/ui/frontend/.next/**',
                ':(exclude)**/node_modules/**', 
                ':(exclude)**/__pycache__/**',
                ':(exclude)**/*.pyc',
                ':(exclude)**/cache/**'
            )
            console.print(f"üìã Got diff between {self.base_branch}...{current_branch} ({len(diff_output)} chars)")
            
            # If diff is empty, check for uncommitted changes
            if not diff_output.strip():
                console.print("‚ö†Ô∏è No committed changes found, checking for uncommitted changes...")
                # Check for uncommitted changes (staged + unstaged)
                staged_diff = self.repo.git.diff('--cached')
                unstaged_diff = self.repo.git.diff()
                
                console.print(f"üìã Staged diff: {len(staged_diff)} chars, Unstaged diff: {len(unstaged_diff)} chars")
                
                if staged_diff or unstaged_diff:
                    diff_output = f"# Staged changes:\n{staged_diff}\n\n# Unstaged changes:\n{unstaged_diff}"
                    console.print(f"üìã Found uncommitted changes for review ({len(diff_output)} chars total)")
                else:
                    diff_output = "No code changes detected. Please ensure the implementer has committed their work."
            
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error getting git diff: {e}")
            # Enhanced fallback: try simpler three-dot syntax
            try:
                diff_output = self.repo.git.diff(f'{self.base_branch}...HEAD')
                console.print(f"üìã Using simpler three-dot diff as fallback ({len(diff_output)} chars)")
                if not diff_output.strip():
                    diff_output = self.repo.git.diff('--cached')  # staged changes
                    if not diff_output.strip():
                        diff_output = self.repo.git.diff()  # unstaged changes
                        if not diff_output.strip():
                            diff_output = "No code changes detected. Implementation may not have been completed or committed."
            except Exception as fallback_error:
                console.print(f"‚ö†Ô∏è All diff methods failed: {fallback_error}")
                diff_output = f"Error retrieving code changes: {str(e)}\nPlease provide the diff manually for review."
        
        # Handle large diffs by saving to file
        console.print(f"üìã Preparing review with diff length: {len(diff_output)} characters")
        
        diff_file_path = None
        diff_content_for_prompt = diff_output
        
        # If diff is large (>10KB), save to file and tell reviewer to read it
        if len(diff_output) > 10000:
            console.print("üìã Large diff detected - saving to temp file for reviewer")
            try:
                # Save diff to temp file in session directory
                diff_file_path = self.session_dir / "current_diff.txt"
                with open(diff_file_path, 'w') as f:
                    f.write(diff_output)
                console.print(f"üìÅ Saved diff to: {diff_file_path}")
                
                # Replace diff content in prompt with file reference
                diff_content_for_prompt = f"[DIFF TOO LARGE - {len(diff_output)} chars]\nPlease use the Read tool to view: {diff_file_path}"
            except Exception as e:
                console.print(f"‚ö†Ô∏è Failed to save diff to file: {e}")
                # Fall back to including truncated diff
                diff_content_for_prompt = diff_output[:5000] + f"\n\n... [TRUNCATED - showing first 5KB of {len(diff_output)} total chars] ...\n\nPlease use git diff command for full changes."

        options = ClaudeCodeOptions(
            system_prompt=self.reviewer_prompt,
            max_turns=20,  # Increased to ensure reviewer can complete tool use and analysis
            allowed_tools=["Read", "Grep", "Bash", "LS"],
            model="claude-sonnet-4-20250514",
            cwd=str(self.repo_path),  # Set working directory to repo
            permission_mode="acceptEdits"  # Allow reviewing files
        )
        
        review_prompt = f"""
Please review this implementation:

**Original Task**: {self.task}

**Implementation Details**: {json.dumps(implementation_data, indent=2)}

**Git Diff**:
```diff
{diff_content_for_prompt}
```

‚ö†Ô∏è CRITICAL REVIEW CHECKLIST:

## 1. PLANNING PHASE VERIFICATION (GATE CHECK)
‚òê Did implementer show evidence of discovery phase?
‚òê Were patterns studied with 3+ examples?
‚òê Was parallel execution used for efficiency?
‚òê Is there a documented implementation plan?

IF ANY UNCHECKED ‚Üí REJECT with reason: "No planning phase completed"

## 2. ORPHAN CODE CHECK (MANDATORY) 
For EVERY new function in the diff:
- Use Grep to find where it's called (LIMIT: Check top 3-5 functions only to avoid running out of turns)
- If NOT called anywhere ‚Üí IMMEDIATE REJECT
- Document the caller in your review
- If you're running low on turns, prioritize outputting the JSON review

## 3. CRITICAL VALIDATIONS
‚òê Task actually solved (not just similar functionality)
‚òê No TODO/FIXME comments remain
‚òê No unused imports/variables/functions
‚òê Follows discovered patterns exactly
‚òê All integration points connected

## 4. REVIEW APPROACH
Prioritize in this order:
1. **Correctness**: Does it solve the exact problem?
2. **Completeness**: Are all parts integrated?
3. **Patterns**: Does it match existing code?
4. **Quality**: Is it clean and maintainable?

‚ö†Ô∏è **TURN MANAGEMENT**: If you reach turn 10+ and haven't output JSON yet, immediately output your review JSON with current findings.

## 5. IF NO DIFF AVAILABLE
‚ö° **PARALLEL EXECUTION**: For maximum efficiency, whenever you need to perform multiple independent operations, invoke all relevant tools simultaneously rather than sequentially.

Use these tools in parallel:
- `git status` - check for uncommitted changes
- `git log --oneline -5` - check recent commits
- `git diff --cached` - check staged changes
- Read modified files directly

Provide your structured JSON review. Be SPECIFIC about issues.
If rejecting, provide EXACT fixes needed (with file:line references).
"""
        
        if diff_output and len(diff_output.strip()) > 0:
            console.print("‚úÖ Diff contains actual code changes")
        else:
            console.print("‚ö†Ô∏è Diff is empty or contains no changes")
        
        full_response = ""
        async with ClaudeSDKClient(options=options) as client:
            await client.query(review_prompt)
            
            async for message in client.receive_response():
                if hasattr(message, 'content'):
                    for block in message.content:
                        if hasattr(block, 'text'):
                            text = block.text
                            print(text, end='', flush=True)
                            full_response += text
                        elif hasattr(block, 'tool_use'):
                            # Add line breaks before tool usage for better readability
                            tool_break = f"\n\n[Tool: {block.tool_use.name}]\n"
                            print(tool_break, end='', flush=True)
                            full_response += tool_break
                        elif hasattr(block, 'tool_result'):
                            # Add line break after tool results
                            result_break = "\n\n"
                            print(result_break, end='', flush=True) 
                            full_response += result_break
        
        # Log the conversation
        self._log_conversation("reviewer", review_prompt, full_response)
        
        # Extract structured output
        try:
            import re
            json_match = re.search(r'```json:review\s*\n(.*?)\n```', full_response, re.DOTALL)
            if json_match:
                review_data = json.loads(json_match.group(1))
                console.print(f"‚úÖ Reviewer completed with approval: {review_data.get('approval', False)}")
                return review_data
            else:
                console.print("‚ö†Ô∏è No structured review found - response may be incomplete!")
                console.print(f"üìã Response length: {len(full_response)} characters")
                console.print(f"üìã Response preview: {full_response[:235] if len(full_response) > 235 else full_response}")
                
                # Check if response seems cut off mid-tool use
                if "Let me" in full_response[-100:] or "I'll" in full_response[-100:]:
                    return {"approval": False, "overall_assessment": "Review incomplete - response cut off during tool use", "concerns": ["Reviewer was cut off mid-analysis, likely hit token or tool use limit"]}
                # If response looks truncated (ends without proper JSON), indicate incomplete review
                elif len(full_response) > 1000 and not full_response.endswith('```'):
                    return {"approval": False, "overall_assessment": "Review incomplete - response was truncated", "concerns": ["Reviewer response was cut off mid-analysis"]}
                else:
                    return {"approval": False, "overall_assessment": "Review failed - no structured output", "concerns": ["No structured review output provided"]}
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error parsing review output: {e}")
            console.print(f"üìã Response length: {len(full_response)} characters")
            console.print("üìã Response preview:", full_response[-200:] if len(full_response) > 200 else full_response)
            return {"approval": False, "overall_assessment": f"Review failed - parsing error: {e}", "concerns": [f"Failed to parse review output: {e}"]}
    
    async def _run_test_planner(self, implementation_data: Dict[str, Any], review_data: Dict[str, Any]) -> Dict[str, Any]:
        """Run test planner agent session."""
        console.print("üìã Starting Test Planner Agent...")
        
        # Get git diff for test planning context
        current_branch = self.repo.active_branch.name
        diff_output = ""
        
        try:
            # Get diff for understanding what was implemented
            diff_output = self.repo.git.diff(
                f'{self.base_branch}...{current_branch}',
                '--', '.',
                ':(exclude)wizardry/ui/frontend/.next/**',
                ':(exclude)**/node_modules/**', 
                ':(exclude)**/__pycache__/**',
                ':(exclude)**/*.pyc',
                ':(exclude)**/cache/**'
            )
            console.print(f"üìã Got diff for test planning ({len(diff_output)} chars)")
            
            if not diff_output.strip():
                # Check for uncommitted changes as fallback
                staged_diff = self.repo.git.diff('--cached')
                unstaged_diff = self.repo.git.diff()
                if staged_diff or unstaged_diff:
                    diff_output = f"# Staged changes:\n{staged_diff}\n\n# Unstaged changes:\n{unstaged_diff}"
                else:
                    diff_output = "No code changes detected for test planning."
            
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error getting git diff for test planner: {e}")
            diff_output = f"Error retrieving code changes: {str(e)}"

        # Handle large diffs by saving to file
        diff_file_path = None
        diff_content_for_prompt = diff_output
        
        if len(diff_output) > 8000:
            console.print("üìã Large diff detected - saving to temp file for test planner")
            try:
                diff_file_path = self.session_dir / "test_planning_diff.txt"
                with open(diff_file_path, 'w') as f:
                    f.write(diff_output)
                console.print(f"üìÅ Saved diff to: {diff_file_path}")
                
                diff_content_for_prompt = f"[DIFF TOO LARGE - {len(diff_output)} chars]\nPlease use the Read tool to view: {diff_file_path}"
            except Exception as e:
                console.print(f"‚ö†Ô∏è Failed to save diff to file: {e}")
                diff_content_for_prompt = diff_output[:4000] + f"\n\n... [TRUNCATED - showing first 4KB of {len(diff_output)} total chars] ..."

        options = ClaudeCodeOptions(
            system_prompt=self.test_planner_prompt,
            max_turns=12,
            allowed_tools=["Read", "Grep", "Bash", "LS"],
            model="claude-sonnet-4-20250514",
            cwd=str(self.repo_path),
            permission_mode="acceptEdits"
        )
        
        test_plan_prompt = f"""
üß™ TEST PLAN GENERATION - Create comprehensive testing strategy

**Original Task**: {self.task}

**What Was Built**: 
{json.dumps(implementation_data.get('rationale', 'See implementation details'), indent=2)}

**Review Approval**: {review_data.get('approval', False)}
**Review Strengths**: {review_data.get('strengths', [])}

**Code Changes**:
```diff
{diff_content_for_prompt}
```

## YOUR MISSION: Create a test plan that gives 95%+ confidence

### PHASE 1: UNDERSTAND THE IMPLEMENTATION
‚ö° **PARALLEL EXECUTION**: For maximum efficiency, whenever you need to perform multiple independent operations, invoke all relevant tools simultaneously rather than sequentially. 

First, analyze what was actually built (execute these in parallel):
- Read the diff to understand code changes
- Identify new functions and their purposes  
- Map the user journey from the implementation
- Note integration points with existing features
- Grep for related test patterns in existing codebase
- Read README.md to understand application context

### PHASE 2: DESIGN TEST COVERAGE

#### Critical Path Tests (MUST PASS):
1. **Happy Path**: Primary user flow works perfectly
2. **Data Persistence**: Changes survive refresh/reload
3. **Integration**: Works with existing features

#### Reliability Tests:
4. **Network Issues**: Offline/slow connection handling
5. **Concurrent Use**: Multiple tabs/users
6. **Extended Sessions**: Long-running stability

#### Boundary Tests:
7. **Input Validation**: Edge cases, limits, invalid data
8. **Browser Compatibility**: Chrome, Firefox, Safari, Edge
9. **Responsive Design**: Mobile, tablet, desktop

#### Performance Tests:
10. **Load Time**: Initial and interaction performance
11. **Scale**: Handle expected data volumes

### PHASE 3: CREATE ACTIONABLE TESTS

For EACH test, specify:
- EXACT UI elements to interact with ("Click the blue 'Submit' button in top-right")
- PRECISE expected behavior ("Modal appears within 2 seconds")
- CLEAR success criteria ("Data appears in table with green checkmark")
- SPECIFIC failure indicators ("Red error message or spinner lasting >5 seconds")

### IMPORTANT CONSTRAINTS:
- Tests must be executable through UI only (no backend access)
- Non-technical users should understand instructions
- Include keyboard/accessibility testing
- Cover both positive and negative scenarios
- Test recovery from errors

Provide:
1. JSON metadata (feature_name, complexity, estimated_time, etc.)
2. Detailed markdown test plan with numbered steps
3. Confidence score after all tests pass

Remember: If someone completes ALL your tests successfully, they should be confident deploying to production.
"""
        
        full_response = ""
        async with ClaudeSDKClient(options=options) as client:
            await client.query(test_plan_prompt)
            
            async for message in client.receive_response():
                if hasattr(message, 'content'):
                    for block in message.content:
                        if hasattr(block, 'text'):
                            text = block.text
                            print(text, end='', flush=True)
                            full_response += text
                        elif hasattr(block, 'tool_use'):
                            tool_break = f"\n\n[Tool: {block.tool_use.name}]\n"
                            print(tool_break, end='', flush=True)
                            full_response += tool_break
                        elif hasattr(block, 'tool_result'):
                            result_break = "\n\n"
                            print(result_break, end='', flush=True) 
                            full_response += result_break
        
        # Log the conversation
        self._log_conversation("test_planner", test_plan_prompt, full_response)
        
        # Save the full test plan to a file for easy access
        try:
            test_plan_file = self.session_dir / "test_plan.md"
            with open(test_plan_file, 'w') as f:
                f.write(full_response)
            console.print(f"üìã Test plan saved to: {test_plan_file}")
        except Exception as e:
            console.print(f"‚ö†Ô∏è Failed to save test plan to file: {e}")
        
        # Extract structured output
        try:
            import re
            json_match = re.search(r'```json:testplan\s*\n(.*?)\n```', full_response, re.DOTALL)
            if json_match:
                test_plan_data = json.loads(json_match.group(1))
                console.print(f"‚úÖ Test Planner completed - Feature: {test_plan_data.get('feature_name', 'Unknown')}")
                console.print(f"üìã Complexity: {test_plan_data.get('test_complexity', 'Unknown')}, Est. Time: {test_plan_data.get('estimated_test_time', 'Unknown')}")
                
                # Add the full response as test plan content
                test_plan_data["test_plan_content"] = full_response
                test_plan_data["test_plan_generated"] = True
                
                return test_plan_data
            else:
                console.print("‚ö†Ô∏è No structured test plan output found")
                return {
                    "test_plan_generated": False, 
                    "error": "No structured output found",
                    "test_plan_content": full_response
                }
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error parsing test plan output: {e}")
            return {
                "test_plan_generated": False, 
                "error": f"Parsing error: {e}",
                "test_plan_content": full_response
            }
    
    def _create_pr(self):
        """Create GitHub PR for the implementation."""
        current_branch = self.repo.active_branch.name
        
        pr_title = f"ü§ñ {self.task[:60]}{'...' if len(self.task) > 60 else ''}"
        pr_body = f"""## Automated Workflow Implementation

**Task**: {self.task}
**Workflow ID**: {self.workflow_id}
**Base Branch**: {self.base_branch}  
**Implementation Branch**: {current_branch}

## Agent Workflow
This PR was created by the Wizardry multi-agent workflow system:
- **Implementer**: Analyzed codebase and implemented solution
- **Reviewer**: Reviewed code quality and approved changes

## Transcripts
Full agent conversations available at: `/tmp/wizardry-sessions/{self.workflow_id}/transcripts/`

---
ü§ñ Generated by Wizardry Agent Orchestrator"""
        
        try:
            # Use gh CLI to create PR
            import subprocess
            result = subprocess.run([
                "gh", "pr", "create", 
                "--base", self.base_branch,
                "--head", current_branch,
                "--title", pr_title,
                "--body", pr_body
            ], cwd=self.repo_path, capture_output=True, text=True)
            
            if result.returncode == 0:
                console.print("‚úÖ Pull request created successfully!")
                console.print(f"PR URL: {result.stdout.strip()}")
                return result.stdout.strip()
            else:
                console.print(f"‚ùå Failed to create PR: {result.stderr}")
                return None
        except Exception as e:
            console.print(f"‚ùå Error creating PR: {e}")
            return None
    
    async def _sync_to_original_repo(self):
        """Sync changes from workspace repo back to original repo if needed."""
        if self.repo_path == self.original_repo_path:
            # No workspace copy - nothing to sync
            return
            
        try:
            import subprocess
            import shutil
            current_branch = self.repo.active_branch.name
            
            console.print(f"üîÑ Syncing changes from workspace to original repo...")
            
            # Switch to original repo and create/update the branch
            original_repo = Repo(self.original_repo_path)
            
            # Fetch the workspace repo as a remote and merge the branch
            workspace_remote_name = f"workspace_{self.workflow_id[:8]}"
            
            # Add workspace as temporary remote
            result = subprocess.run([
                "git", "-C", str(self.original_repo_path), "remote", "add", 
                workspace_remote_name, str(self.repo_path)
            ], capture_output=True, text=True)
            
            # Fetch the branch from workspace
            result = subprocess.run([
                "git", "-C", str(self.original_repo_path), "fetch", 
                workspace_remote_name, current_branch
            ], check=True, capture_output=True, text=True)
            
            # Create/update the branch in original repo
            result = subprocess.run([
                "git", "-C", str(self.original_repo_path), "branch", "-f",
                current_branch, f"{workspace_remote_name}/{current_branch}"
            ], check=True, capture_output=True, text=True)
            
            # Clean up the temporary remote
            subprocess.run([
                "git", "-C", str(self.original_repo_path), "remote", "remove", 
                workspace_remote_name
            ], capture_output=True, text=True)
            
            console.print(f"‚úÖ Successfully synced branch '{current_branch}' to original repo")
            
        except subprocess.CalledProcessError as e:
            console.print(f"‚ö†Ô∏è Failed to sync to original repo: {e.stderr}")
            console.print(f"üìã Manual sync required - check workspace at: {self.repo_path}")
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error during repo sync: {e}")
    
    def _cleanup_worktree(self):
        """Clean up the git worktree after workflow completion."""
        if self.repo_path == self.original_repo_path:
            return  # No worktree to clean up
            
        try:
            import subprocess
            worktree_branch = f"wizardry-{self.workflow_id}"
            
            # Remove worktree
            result = subprocess.run([
                "git", "-C", str(self.original_repo_path),
                "worktree", "remove", str(self.repo_path)
            ], check=True, capture_output=True, text=True)
            
            console.print(f"üßπ Cleaned up worktree: {self.repo_path}")
            
            # Optionally remove the branch (but keep it for reference)
            # subprocess.run(["git", "-C", str(self.original_repo_path), "branch", "-D", worktree_branch])
            
        except subprocess.CalledProcessError as e:
            console.print(f"‚ö†Ô∏è Failed to clean up worktree: {e.stderr}")
        except Exception as e:
            console.print(f"‚ö†Ô∏è Error during worktree cleanup: {e}")
    
    @staticmethod
    def archive_session(session_id: str, cleanup_branch: bool = True) -> bool:
        """Archive a session and clean up all associated resources."""
        registry_file = Path("/tmp/wizardry-sessions/registry.json")
        archived_dir = Path("/tmp/wizardry-sessions/archived")
        
        try:
            # Load session registry
            if not registry_file.exists():
                console.print(f"‚ùå Registry file not found")
                return False
                
            with open(registry_file, 'r') as f:
                registry = json.load(f)
                
            if session_id not in registry:
                console.print(f"‚ùå Session {session_id} not found in registry")
                return False
                
            session = registry[session_id]
            session_dir = Path(session["workspace_path"])
            original_repo_path = session.get("repo_path", "")
            workspace_repo_path = session.get("workspace_repo_path", "")
            
            console.print(f"üóÇÔ∏è Archiving session {session_id}...")
            
            # 1. Move session to archived directory
            archived_dir.mkdir(exist_ok=True, parents=True)
            archived_session_dir = archived_dir / session_id
            
            if session_dir.exists():
                import shutil
                if archived_session_dir.exists():
                    shutil.rmtree(archived_session_dir)
                shutil.move(str(session_dir), str(archived_session_dir))
                console.print(f"üì¶ Moved session files to: {archived_session_dir}")
            
            # 2. Clean up branch if requested and if we have repo info
            if cleanup_branch and original_repo_path and session.get("status") in ["completed", "failed", "terminated"]:
                try:
                    # Find the branch name from session logs or use pattern
                    branch_pattern = f"wizardry-*{session_id[-6:]}"  # Use last 6 chars of session ID
                    
                    import subprocess
                    # List branches matching the pattern
                    result = subprocess.run([
                        "git", "-C", original_repo_path, "branch", "--list", branch_pattern
                    ], capture_output=True, text=True)
                    
                    if result.returncode == 0 and result.stdout.strip():
                        branches = [b.strip().lstrip('* ') for b in result.stdout.strip().split('\n')]
                        for branch in branches:
                            if branch and branch != session.get("base_branch"):
                                # Delete the branch
                                result = subprocess.run([
                                    "git", "-C", original_repo_path, "branch", "-D", branch
                                ], capture_output=True, text=True)
                                
                                if result.returncode == 0:
                                    console.print(f"üóëÔ∏è Deleted branch: {branch}")
                                else:
                                    console.print(f"‚ö†Ô∏è Failed to delete branch {branch}: {result.stderr}")
                                    
                except Exception as e:
                    console.print(f"‚ö†Ô∏è Error cleaning up branch: {e}")
            
            # 3. Clean up git worktree if it exists and is different from original
            if workspace_repo_path and workspace_repo_path != original_repo_path:
                workspace_path = Path(workspace_repo_path)
                if workspace_path.exists() and workspace_path != Path(original_repo_path):
                    try:
                        import subprocess
                        # First try to remove the worktree using git worktree remove
                        result = subprocess.run([
                            "git", "-C", original_repo_path, "worktree", "remove", str(workspace_path)
                        ], capture_output=True, text=True)
                        
                        if result.returncode == 0:
                            console.print(f"üóëÔ∏è Removed git worktree: {workspace_path}")
                        else:
                            # If git worktree remove fails, try force removal and then manual cleanup
                            console.print(f"‚ö†Ô∏è Git worktree remove failed: {result.stderr}")
                            
                            # Force remove from git's worktree list
                            subprocess.run([
                                "git", "-C", original_repo_path, "worktree", "remove", "--force", str(workspace_path)
                            ], capture_output=True, text=True)
                            
                            # Manually clean up directory if it still exists
                            if workspace_path.exists():
                                import shutil
                                shutil.rmtree(workspace_path)
                                console.print(f"üóëÔ∏è Manually cleaned up workspace directory: {workspace_path}")
                    except Exception as e:
                        console.print(f"‚ö†Ô∏è Failed to clean up git worktree: {e}")
                        # Fallback to manual directory cleanup
                        try:
                            if workspace_path.exists():
                                import shutil
                                shutil.rmtree(workspace_path)
                                console.print(f"üóëÔ∏è Fallback cleanup of workspace directory: {workspace_path}")
                        except Exception as fallback_e:
                            console.print(f"‚ö†Ô∏è Fallback cleanup also failed: {fallback_e}")
            
            # 4. Remove from active registry
            del registry[session_id]
            with open(registry_file, 'w') as f:
                json.dump(registry, f, indent=2)
            
            console.print(f"‚úÖ Session {session_id} archived successfully")
            return True
            
        except Exception as e:
            console.print(f"‚ùå Error archiving session {session_id}: {e}")
            return False
    
    async def run_workflow(self) -> bool:
        """Run the complete workflow."""
        console.print(f"üöÄ Starting Wizardry workflow: {self.workflow_id}")
        console.print(f"üìã Task: {self.task}")
        console.print(f"üåø Base branch: {self.base_branch}")
        
        try:
            # Workspace already set up by _setup_workspace_repo() in constructor
            self._register_session()
            
            # Run implementer
            with console.status("üîß Implementer working..."):
                implementation_data = await self._run_implementer()
            
            if not implementation_data.get("ready_for_review", False):
                console.print("‚ùå Implementer did not complete successfully")
                return False
            
            # Run reviewer  
            with console.status("üîç Reviewer analyzing..."):
                review_data = await self._run_reviewer(implementation_data)
            
            # Check approval
            max_iterations = 3
            iteration = 1
            
            while not review_data.get("approval", False) and iteration <= max_iterations:
                console.print(f"‚ùå Review rejected (iteration {iteration}/{max_iterations})")
                console.print("Feedback:", review_data.get("suggested_fixes", []))
                
                # Re-run implementer with feedback
                console.print(f"üîÑ Re-running implementer with feedback (iteration {iteration + 1})")
                
                # Create feedback prompt for implementer
                feedback_prompt = f"""
‚ö†Ô∏è REVISION REQUIRED - Iteration {iteration + 1}/{max_iterations}

The reviewer found issues that must be fixed before approval.

**Original Task**: {self.task}

**What You Implemented Previously**:
{json.dumps(implementation_data, indent=2)}

**CRITICAL ISSUES TO FIX**:

## Reviewer's Specific Concerns:
{chr(10).join(f"- ‚úó {concern}" for concern in review_data.get("concerns", []))}

## Required Fixes (MUST complete ALL):
{chr(10).join(f"- [ ] {fix}" for fix in review_data.get("suggested_fixes", []))}

## Additional Context:
- Overall Assessment: {review_data.get("overall_assessment", "Not provided")}
- Approval Status: {review_data.get("approval", False)}

## YOUR REVISION APPROACH:

1. **UNDERSTAND THE ISSUES**:
   - Read each concern carefully
   - Use Grep/Read to verify the problems
   - Don't assume - verify everything

2. **TRACK YOUR FIXES**:
   Mark each fix as you complete it:
   - [‚úì] Fixed orphaned function by adding caller at line X
   - [‚úì] Removed TODO comment from file Y
   - [‚úì] Updated pattern to match existing code

3. **VERIFY BEFORE COMMITTING**:
   - Run tests if available
   - Check no new orphaned functions
   - Ensure all fixes are addressed

4. **COMMIT AND DOCUMENT**:
   - Use clear commit message: "fix: address review feedback - [summary]"
   - Update JSON output with changes made
   - Document what was fixed and how

REMEMBER:
- The reviewer will check if you ACTUALLY fixed the issues
- Don't just claim fixes - implement them
- Every function needs a caller
- Follow existing patterns exactly
- NO TODO/FIXME comments

**CRITICAL**: You MUST provide your response in this exact format:

```json:implementation
{
  "rationale": "Summary of what you fixed and changed",
  "files_modified": ["list", "of", "files", "changed"],
  "patterns_followed": {"pattern": "explanation"},
  "call_graph": {"function": ["caller1:file.ts:line"]},
  "patterns_studied": {"Pattern": "How you studied existing code"},
  "integration_points": ["How the code integrates"],
  "user_journey": ["Step by step user flow"],
  "confidence": 9,
  "testing_notes": "How you verified the fixes",
  "commit_hash": "abc12345",
  "committed": true,
  "ready_for_review": true
}
```
"""
                
                with console.status(f"üîß Implementer fixing issues (iteration {iteration + 1})..."):
                    implementation_data = await self._run_implementer_with_feedback(feedback_prompt)
                
                if not implementation_data.get("ready_for_review", False):
                    console.print(f"‚ùå Implementer failed to address feedback (iteration {iteration + 1})")
                    break
                
                # Re-review the updated implementation
                with console.status(f"üîç Reviewer re-analyzing (iteration {iteration + 1})..."):
                    review_data = await self._run_reviewer(implementation_data)
                
                iteration += 1
            
            if review_data.get("approval", False):
                console.print("‚úÖ Review approved!")

                # Run test planner after successful review
                console.print("üìã Starting test planning phase...")
                test_plan_data = {}
                try:
                    with console.status("üìã Test Planner analyzing implementation..."):
                        test_plan_data = await self._run_test_planner(implementation_data, review_data)
                    
                    if test_plan_data.get("test_plan_generated", False):
                        console.print("‚úÖ Test plan generated successfully!")
                        console.print(f"üìã Feature: {test_plan_data.get('feature_name', 'Unknown')}")
                        console.print(f"üìã Test complexity: {test_plan_data.get('test_complexity', 'Unknown')}")
                        console.print(f"üìã Estimated test time: {test_plan_data.get('estimated_test_time', 'Unknown')}")
                    else:
                        console.print("‚ö†Ô∏è Test plan generation had issues, but workflow continues")
                except Exception as e:
                    console.print(f"‚ö†Ô∏è Test planning failed: {e}")                
                # Sync changes back to original repo if we're using a workspace copy
                await self._sync_to_original_repo()
                
                # Create PR
                pr_url = self._create_pr()
                
                # Keep worktree for diff viewing - cleanup happens during session archive
                
                console.print("üéâ Workflow completed successfully!")
                if test_plan_data.get("test_plan_generated", False):
                    console.print("üìã Test plan is ready! Check the UI for detailed testing instructions.")
                success = True
            else:
                console.print("‚ùå Workflow failed after max iterations")
                success = False
                
        except Exception as e:
            console.print(f"‚ùå Workflow error: {e}")
            success = False
        finally:
            # Worktree cleanup handled during session archive, not here
                
            # Update session status based on actual result
            if success:
                status = "completed" if ('test_plan_data' in locals() and test_plan_data.get("test_plan_generated", False)) else "completed"
            else:
                status = "failed"
            self._update_session_status(status)
            console.print(f"üìä Session status updated: {status}")
        
        return success
    
    def _update_session_status(self, status: str):
        """Update session status in registry and broadcast to WebSocket clients."""
        registry_file = Path("/tmp/wizardry-sessions/registry.json")
        try:
            with open(registry_file, 'r') as f:
                registry = json.load(f)
            
            if self.workflow_id in registry:
                registry[self.workflow_id]["status"] = status
                
                with open(registry_file, 'w') as f:
                    json.dump(registry, f, indent=2)
                
                # Broadcast status update to WebSocket clients
                self._broadcast_status_update(status)
        except Exception:
            pass  # Session tracking is not critical

    def _broadcast_status_update(self, status: str):
        """Broadcast status update to WebSocket clients via API."""
        try:
            # Get API URL from environment or use default
            api_url = os.environ.get('WIZARDRY_API_URL', 'http://localhost:8001/api')
            
            with httpx.Client(timeout=2.0) as client:
                client.post(
                    f"{api_url}/broadcast/status-update",
                    json={
                        "session_id": self.workflow_id,
                        "status": status
                    }
                )
        except Exception:
            pass  # Broadcasting is not critical, don't fail the workflow


async def run_orchestrator(repo_path: str, branch: str, task: str) -> bool:
    """Entry point for running orchestrated workflow."""
    orchestrator = WorkflowOrchestrator(repo_path, branch, task)
    return await orchestrator.run_workflow()
